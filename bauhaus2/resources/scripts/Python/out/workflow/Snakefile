# ---------------------------------------------------------------------------------------------------
# stub.py : set up runtime/stdlib and initialize env for the snakemake workflow

from bauhaus2.runtime import *

shell.executable("/bin/bash")
prefix = open("prefix.sh").read()
shell.prefix(prefix)

# ---------------------------------------------------------------------------------------------------
# summarize-mappings.snake: analyze mapping results, generating plots and tables.

mapped_subreads = \
    { c : expand("conditions/{condition}/mapped/mapped.alignmentset.xml", condition=c)
      for c in ct.conditions }


# -- Target rules --

rule summarize_mappings:
    input:
        pbiPlots="reports/PbiPlots/report.json",
        pbiSampledPlots="reports/PbiSampledPlots/report.json",
        libDiagnosticPlots="reports/LibDiagnosticPlots/report.json",
        readPlots="reports/ReadPlots/report.json",
        constantArrowPlots="reports/ConstantArrowFishbonePlots/report.json",
        ZMWstsPlots="reports/ZMWstsPlots/report.json"

rule PbiPlots:
    input: mapped_subreads.values()
    output: "reports/PbiPlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/PbiPlots.R
        """

rule PbiSampledPlots:
    input: mapped_subreads.values()
    output: "reports/PbiSampledPlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/PbiSampledPlots.R
        """

rule LibDiagnosticPlots:
    input: mapped_subreads.values()
    output: "reports/LibDiagnosticPlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/LibDiagnosticPlots.R
        """

rule ReadPlots:
    input: mapped_subreads.values()
    output: "reports/ReadPlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/ReadPlots.R
        """

rule ZMWstsPlots:
    input: mapped_subreads.values()
    output: "reports/ZMWstsPlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/ZMWstsPlots.R
        """# ---------------------------------------------------------------------------------------------------
# make-mapping-metrics-csv.snake: make a csv summary of import metrics, by ZMW

from bauhaus2.runtime import ct

assert ct.inputsAreMapped, "make-mapping-metrics-csv.snake is a task for mapped data"

local_alignmentset = \
    { c : expand("conditions/{condition}/mapped/mapped.alignmentset.xml", condition=c)
      for c in ct.conditions }

rule make_mapping_metrics_csv_one_condition:
	input: 
		aset = "conditions/{condition}/mapped/mapped.alignmentset.xml",
		arrow_csv = "reports/ConstantArrowFishbonePlots/errormode_{condition}.csv"
	output: metrics_csv = "reports/MappingMetricsCsv/mapped-metrics_{condition}.csv"
	shell:
		"""
			module add smrttools/mainline
			python MakeMappingMetricsCsv.py --aset {input.aset} --arrow_csv {input.arrow_csv} --output {output.metrics_csv}
		"""
# ---------------------------------------------------------------------------------------------------
# constant-arrow.snake: fit constant arrow model, generating csv file of errormode,
# and make Fishbone plots using the csv file.

mapped_subreads = \
    { c : expand("conditions/{condition}/mapped/mapped.alignmentset.xml", condition=c)
      for c in ct.conditions }


# -- Target rules --

rule constant_arrow_plots:
    input:
        constantArrowPlots="reports/ConstantArrowFishbonePlots/report.json"

rule ConstantArrowPlots:
    input: "reports/ConstantArrowFishbonePlots/errormode.csv"
    output: "reports/ConstantArrowFishbonePlots/report.json"
    shell:
        """
        Rscript --vanilla scripts/R/FishbonePlots.R
        """
        
rule ConstantArrow:
    input: mapped_subreads.values()
    output: "reports/ConstantArrowFishbonePlots/errormode.csv"
    shell:
        """
        Rscript --vanilla scripts/R/constant_arrow.R
        """
# ---------------------------------------------------------------------------------------------------
# map-unrolledNoHQ-smrtlink.snake: map subreads using a SMRTLink server, via pbservice call

local_alignmentsets = \
    { c : expand("conditions/{condition}/mapped/mapped.alignmentset.xml", condition=c)
      for c in ct.conditions }
      
# -- Target rules --

rule map_unrolledNoHQ:
    input: local_alignmentsets.values()

# -- Worker rules --

sl_host=config["bh2.smrtlink.host"]
sl_port=config["bh2.smrtlink.services_port"]


# -- Hack: we need to workaround a problem with SMRTLink where jobs can't
# -- be submitted simultaneously.  To achieve these, we run all submits
# -- on the head node (making this a localrule), and then we use a
# -- semaphore to serialize executions of `pbservice run-pipeline`

localrules: map_unrolledNoHQ_smrtlink_launch

rule map_unrolledNoHQ_smrtlink_launch:
    input:
        subreadSet="conditions/{condition}/subreads/input.subreadset.xml",
        referenceSet=lambda wc: ct.referenceSet(wc.condition)
    output:
        pbserviceInfo="conditions/{condition}/mapped/pbservice.log"
    params:
        presetXmlFile="extras/pbsmrtpipe-unrolled-mappings-preset.xml",
        jobid = "Bauhaus2_Job_{condition}"
    shell:
        """
        module add parallel

        # Call pbservice; bail out on failure
        sem --no-notice --id smrtlink-siv-semaphore -j1 --fg \
        pbservice run-pipeline pbsmrtpipe.pipelines.sa3_ds_align_unrolled \
          --host {sl_host} --port {sl_port} \
          -e {input.subreadSet} \
          -e {input.referenceSet} \
          --preset-xml {params.presetXmlFile} --job-title {params.jobid} \
        > {output}
        """

rule map_unrolledNoHQ_smrtlink_poll:
    input: "conditions/{condition}/mapped/pbservice.log"
    output:
        flagFile="conditions/{condition}/mapped/job_complete",
        pollFile="conditions/{condition}/mapped/poll.log",
        localJobLink="conditions/{condition}/mapped/job_link"
    run:
        import time, sys, os
        jobId = extractJobId(input[0])
        jobRoot = extractJobPath(input[0])
        os.symlink(jobRoot, output.localJobLink)
        while True:
            shell("pbservice get-job %s --host {sl_host} --port {sl_port} > {output.pollFile}" % jobId)
            status = extractJobStatus(output.pollFile)
            print("Found status: %s" % status)
            if status in JobStatus.FAILED_STATES:
                print("SMRTLink job has failed")
                sys.exit(1)
            elif status in JobStatus.SUCCESS_STATES:
                print("SMRTLink job has succeeded")
                break
            else:
                print("Will sleep")
                time.sleep(90)
                continue
        touchFile(output.flagFile)

rule map_unrolledNoHQ_smrtlink_collect:
    input:
        flagFile="conditions/{condition}/mapped/job_complete",
        pollFile="conditions/{condition}/mapped/poll.log",
        localJobLink="conditions/{condition}/mapped/job_link",
        reference="conditions/{condition}/reference.fasta"
    output:
        "conditions/{condition}/mapped/mapped.alignmentset.xml"
    params:
        locationOfAlignmentSet="tasks/pbalign.tasks.consolidate_alignments-0/combined.alignmentset.xml"
    shell:
        """
        dataset create {output} {input.localJobLink}/{params.locationOfAlignmentSet}
        """
class JobStatus(object):
    VALID_STATES = ("CREATED", "SUBMITTED", "RUNNING", "TERMINATED", "SUCCESSFUL", "FAILED", "UNKNOWN")
    FAILED_STATES = ("TERMINATED", "FAILED", "UNKNOWN")
    SUCCESS_STATES = ("SUCCESSFUL",)

def extractJobId(launchLog):
    import re
    logContent = open(launchLog).read()
    ms = list(re.finditer('JOB SUMMARY:$\n^\s+id: (.*)', logContent, re.MULTILINE))
    if ms:
        return ms[-1].group(1)
    else:
        raise ValueError("Cannot find job ID in pbservice.log")

def extractJobStatus(pollLog):
    import re
    logContent = open(pollLog).read()
    m = re.search('JOB SUMMARY:.*^\s+state: (\w*)', logContent, re.MULTILINE | re.DOTALL)
    if not m or m.group(1) not in JobStatus.VALID_STATES:
        raise ValueError("Cannot find valid job status in pbservice.log")
    else:
        return m.group(1)

def extractJobPath(pollLog):
    import re
    logContent = open(pollLog).read()
    m = re.search('JOB SUMMARY:.*^\s+path: (\S*)', logContent, re.MULTILINE | re.DOTALL)
    if not m:
        raise ValueError("Cannot find SMRTLink job path in pbservice.log")
    else:
        return m.group(1)

def touchFile(fname):
    open(fname, "a").close()
# ---------------------------------------------------------------------------------------------------
# collect-references.snake: hotlink "remote" reference FASTAs into our workflow directory

local_fasta_files = \
    { c : expand("conditions/{condition}/reference.fasta", condition=c)
      for c in ct.conditions }

remote_sts_h5 = \
    { c : ct.inputsH5(c)
      for c in ct.conditions }

rule collect_references:
    input: local_fasta_files.values()

rule collect_reference_one_condition:
    input:
        fasta=lambda wc: ct.reference(wc.condition),
        fai=lambda wc: ct.reference(wc.condition)+".fai",
        sts=lambda wc: remote_sts_h5[wc.condition]
    output:
        fasta="conditions/{condition}/reference.fasta",
        fai="conditions/{condition}/reference.fasta.fai",
        sts="conditions/{condition}/sts.h5"
    shell:
        """
        ln -s {input.fasta} {output.fasta}
        ln -s {input.fai} {output.fai}
        ln -s {input.sts} {output.sts}
        """
# ---------------------------------------------------------------------------------------------------
# scatter-subreads.snake: split subreadsets into smaller chunks for analysis

chunked_subreads = \
    { c : expand("conditions/{condition}/subreads/chunks/input.chunk{chunkNo}.subreadset.xml",
                 condition=c, chunkNo=range(config["bh2.scatter_subreads.chunks_per_condition"]))
      for c in ct.conditions }

# -- Target --

rule chunk_subreads:
    input: listConcat(chunked_subreads.values())

# -- Worker rules --

rule chunk_subreads_one_condition:
    input: lambda wc: local_subreadset[wc.condition]
    output:
        expand("conditions/{{condition}}/subreads/chunks/input.chunk{chunkNo}.subreadset.xml",
               chunkNo=range(config["bh2.scatter_subreads.chunks_per_condition"]))
    params: outdir="conditions/{condition}/subreads/chunks"
    shell:
        """
        dataset split --zmws --targetSize 1 --chunks 8 --outdir {params.outdir} {input}
        """
# ---------------------------------------------------------------------------------------------------
# collect-subreads.snake: hotlink "remote" subreadsets into the workflow directory

assert (not ct.inputsAreMapped), "collect-subreads.snake is for workflows with unmapped data"

local_subreadset = \
    { c : "conditions/{condition}/subreads/input.subreadset.xml".format(condition=c)
      for c in ct.conditions }

remote_subreadsets = \
    { c : ct.inputs(c)
      for c in ct.conditions }

# -- Target --

rule collect_subreads:
    input: local_subreadset.values()

# -- Worker rules ---

rule collect_subreads_one_condition:
    input: lambda wc: remote_subreadsets[wc.condition]
    output: "conditions/{condition}/subreads/input.subreadset.xml"
    shell:
        """
        dataset create {output} {input}
        """
