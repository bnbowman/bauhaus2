#!/usr/bin/env Rscript
# Fit a constant rate arrow model to an alignment/reference and output the
# parameters as a CSV
# Make finshbone plots from a csv file generated by fitting hmm model
# This script combines constant_arrow.R and FishbonePlotsCmdLine.R

library(argparse)
library(data.table, quietly = TRUE)
library(jsonlite, quietly = TRUE)
library(logging)
library(ggplot2)
library(pbbamr)
library(uuid, quietly = TRUE)
library(gridExtra)
library(dplyr, quietly = TRUE)
library(tidyr, quietly = TRUE)
library(unitem)
library(nnet)
library(reshape2)
library(lazyeval)

## FIXME: make a real package
myDir = "./scripts/R"
source(file.path(myDir, "Bauhaus2.R"))

# load sample size for argument, default sample size = 500
parser <- ArgumentParser()
parser$add_argument("--sampleSize", nargs = 1, default = 500, help = "number of samples (ZMWs) for each condition")
try(args <- parser$parse_args())
set.seed(args$seed)

## PARAMETERS THAT SHOULD BECOME TASK OPTIONS
# Used to filter out alignments that don't have enough data for fitting
MIN_ALN_LENGTH = 1000 # Should be larger, but test data was ~930 bp in size and wanted to keep that working.
# Set up sample size of the sampled ZMW for each condition:
SAMPLING_SIZE = args$sampleSize

# Define a basic addition to all plots
plTheme <- theme_bw(base_size = 18)
clScale <- scale_colour_brewer(palette = "Set1")
clFillScale <- scale_fill_brewer(palette = "Set1")
themeTilt = theme(axis.text.x = element_text(size = 11, angle = 45, hjust = 1))
pd <- position_dodge(0.2)
dpi <- 72

# Output column names
# In below column names, ‘X.Insert.Y’ indicates the Y-insertion (read base) rate for a XX or NX context (template)
# Similar for match/mismatch events
csv_names = c(
  "ZMW",
  "SNR.A",
  "SNR.C",
  "SNR.G",
  "SNR.T",
  "A.Insert.A",
  "C.Insert.A",
  "G.Insert.A",
  "T.Insert.A",
  "A.Insert.C",
  "C.Insert.C",
  "G.Insert.C",
  "T.Insert.C",
  "A.Insert.G",
  "C.Insert.G",
  "G.Insert.G",
  "T.Insert.G",
  "A.Insert.T",
  "C.Insert.T",
  "G.Insert.T",
  "T.Insert.T",
  "A.Match.A",
  "C.Match.A",
  "G.Match.A",
  "T.Match.A",
  "A.Match.C",
  "C.Match.C",
  "G.Match.C",
  "T.Match.C",
  "A.Match.G",
  "C.Match.G",
  "G.Match.G",
  "T.Match.G",
  "A.Match.T",
  "C.Match.T",
  "G.Match.T",
  "T.Match.T",
  "A.Dark.A",
  "C.Dark.C",
  "G.Dark.G",
  "T.Dark.T",
  "A.Merge.A",
  "C.Merge.C",
  "G.Merge.G",
  "T.Merge.T",
  "AlnTLength",
  "Time",
  "Iterations"
)

##' Filters out data with large length discrepencies
##' param data Data to be filtered
# Filter out large descrepancies
# The idea is that if a read/template pair differ by more than 20% in length, they are not
# suitable for fitting.
filterData <- function(data) {
  noGood <- function(x) {
    nm = sum(x$read != "-")
    no = sum(x$ref != "-")
    dif = abs(1 - nm / no)
    dif < .3
  }
  Filter(noGood, data)
}

#' Main function to produce CSV given a json file and output path
constantArrow <-
  function(input_aln,
           input_ref,
           Condition,
           report) {
    loginfo("Running Arrow Training.")
    loginfo(paste("Input Aln:", input_aln))
    loginfo(paste("Input Ref:", input_ref))
    loginfo(paste("Requested Sampling Size:", SAMPLING_SIZE))
    loginfo(paste(
      "R_LIBS is: ",
      .libPaths(),
      sep = "",
      collapse = "\n"
    ))

    loginfo(paste("Fasta file:", input_ref))

    # Filter the data set
    ind = loadPBI(input_aln)
    org_size = nrow(ind)
    indFilter = ind[ind$tend - ind$tstart > MIN_ALN_LENGTH,]
    loginfo(paste("Filtered out", org_size - nrow(indFilter), "alignments for being too small for fitting"))

    # Handle case with no valid alignments, write empty CSV
    if (nrow(indFilter) == 0) {
      errormode <- data.frame(matrix(NA, nrow = 0, ncol = length(csv_names)))
      colnames(errormode) <- csv_names
      report$write.table(paste("errormode_", Condition, ".csv", sep = ''),
                         errormode,
                         id = "errormode",
                         title = "Constant Arrow Errormode")
      logging::loginfo(paste("Wrote empty CSV for ", Condition))
      return(0)
    }
    # Decide the sampling size
    ZMWS_TO_SAMPLE = min(nrow(indFilter), as.numeric(SAMPLING_SIZE))
    if (ZMWS_TO_SAMPLE != nrow(indFilter)) {
      sampled_rows = sample(nrow(indFilter), ZMWS_TO_SAMPLE)
    } else {
      sampled_rows = 1:nrow(indFilter)
    }
    loginfo(paste("Rows of Data Used:", length(sampled_rows)))
    sampled_ZMW <- indFilter$hole[sampled_rows]
    indFilter = indFilter[sampled_rows,]
    errormode <-
      data.frame(matrix(NA, nrow = length(sampled_rows), ncol = length(csv_names)))
    colnames(errormode) <- csv_names
    errormode$ZMW = sampled_ZMW


    # Aggregate the pmf matrix
    baseAgg <- function(pmf) {
      pmf = aggregate(pmf[,c(1:4)], list(substr(pmf[, "CTX"], 2, 2)), function(x)
        (0.25 * x[1] + 0.75 * x[2]))
      pmf
    }

    bases <- c("A", "C", "G", "T")
    for (i in 1:nrow(errormode)) {
      if ((i %% 100) == 0) {
        loginfo(paste("Processing Number ", i))
      }
      singleZMW = loadSingleZmwHMMfromBAM(indFilter$offset[i], as.character(indFilter$file[i]), input_ref)
      errormode[i, paste("SNR.", bases, sep = "")] = as.numeric(attributes(singleZMW)[[1]])
      ## Filter out really discordant alignments, they create numeric issues
      singleZMW <- filterData(singleZMW)
      # Handle case where all data is filtered
      if (length(singleZMW) == 0) {
        loginfo("Warning!!! Alignment had all data removed by the length mismatch filter for this run.")
        errormode[i, 1:ncol(errormode)] = NA
      } else {
        # Fit hmm
        fit = hmm(read ~ 1,
                  singleZMW,
                  verbose = FALSE,
                  filter = FALSE,
                  use8Contexts = TRUE,
                  end_dif = 0.005)
        # Summerize model parameters
        predictions <- list()
        for (j in 1:8) {
          predictions[[j]] = predict(fit$models[[j]]$cfit, type = "probs")[1,]
        }
        predictions <- as.data.frame(matrix(unlist(predictions), ncol = 4, byrow = T))
        colnames(predictions) = colnames(fit$pseudoCounts) # copy the outcome names over
        CTX <- fit$sPmf$CTX

        # Dark rate
        darkCols <- which(csv_names == "A.Dark.A"):which(csv_names == "T.Dark.T")
        darkRows <- which(CTX == "NA"):which(CTX == "NT")
        errormode[i, darkCols] = predictions[darkRows, "Delete"]

        # Merge rate
        mergeCols <- which(csv_names == "A.Merge.A"):which(csv_names == "T.Merge.T")
        mergeRows <- which(CTX == "AA"):which(CTX == "TT")
        errormode[i, mergeCols] = predictions[mergeRows, "Delete"] - predictions[darkRows, "Delete"]

        # Match rate
        matchCols <- which(csv_names == "A.Match.A"):which(csv_names == "T.Match.T")
        matchVals <- predictions[, "Match"] * fit$mPmf[, bases]
        matchVals$CTX <- fit$mPmf$CTX
        errormode[i, matchCols] = as.vector(as.matrix(baseAgg(matchVals)[, bases]))

        # When insert a different base, use stick
        stickCols <- which(csv_names == "A.Insert.A"):which(csv_names == "T.Insert.T")  # also includes branch, will replace below
        stickVals <- predictions[, "Stick"] * fit$sPmf[, bases]
        stickVals$CTX <- fit$sPmf$CTX
        errormode[i, stickCols] = as.vector(as.matrix(baseAgg(stickVals)[, bases]))

        # When insert a same base, use branch
        branchCols <- which(csv_names %in% paste(bases, ".Insert.", bases, sep=""))
        branchVals <- predictions[, "Branch"] * fit$bPmf[, bases]
        branchVals$CTX <- fit$bPmf$CTX
        errormode[i, branchCols] = diag(as.matrix(baseAgg(branchVals)[, bases]))


        # Get the alignment length, accounting for any filtered bases
        errormode[i, "AlnTLength"] = sum(sapply(singleZMW, function(x) sum(x$ref != "-")))
        errormode[i, "Time"] = fit$time_s
        errormode[i, "Iterations"] = length(fit$likelihoodHistory)
      }
    }

    report$write.table(paste("errormode_", Condition, ".csv", sep = ''),
                       errormode,
                       id = "errormode",
                       title = "Constant Arrow Errormode")
    logging::loginfo(paste("Wrote CSV for ", Condition))
    return(errormode)
  }

makeFishbonePlots <- function(errormodeList, report, minSample = 20) {
  loginfo("making fishbone plots.")

  # some functions
  concat <- function(...) paste(..., sep = "")
  nam <- function(...) as.name(concat(...))

  breaks = c(0, 1:40)/2
  bases <- c("A", "C", "G", "T")

  massage <- function(df, channel) {
    # filter on data specifically under the expectation of the channel base
    ch <- concat(channel, ".")
    re <- concat("^", channel, "\\.")
    df <- df %>%
      dplyr::select(ZMW, starts_with("SNR."), starts_with(ch)) %>%
      dplyr::rename_(.dots = setNames(names(.), gsub(re, "", names(.))))

    ci95 <- function(var) interp(~ sd(x)/sqrt(n()) * qt(0.975, n() - 1), x = lazy(var))
    mvNams <- function(b, moves) unlist(lapply(c(moves), function(x) concat(x, ".", b)))
    stNams <- function(b, moves) unlist(lapply(c(moves), function(x) concat(x, ".", b, ".", c("mu", "ci"))))

    # collect the results for each outcome base
    dfCh <- NULL

    for (b in bases) {
      # get the transition moves under the regime of expectedBase:observedBase
      moves <- gsub(".[ACGT]$", "", colnames(dplyr::select(df, ends_with(concat(".", b)), -starts_with("SNR"))))

      # get the raw move names
      nms <- mvNams(b, moves)

      # prepare the mean- and 95%ci-accumulating functions
      mus <- lapply(nms, function(x) interp(~ mean(y), y = as.name(x)))
      cis <- lapply(nms, function(x) eval(interp(ci95(y), y = as.name(x))))

      # create the interleaved list of mean- and 95%ci-accumulating functions
      obj <- c(rbind(mus, cis))

      # get the names of the statistical variables
      nms <- stNams(b, moves)

      # add the number of observations to the list of variables (for later filtering)
      obj <- c(obj, ~ n())
      nms <- c(nms, "N")

      # group the samples by their SNR bin and collect summary mean and 95%ci
      suppressWarnings({
        dfStats <- df %>%
          dplyr::select(ZMW, ends_with(concat(".", b))) %>%
          dplyr::mutate_(snr = interp(~ cut(x, breaks), x = nam("SNR.", b))) %>%
          dplyr::select(-starts_with("SNR.")) %>%
          dplyr::group_by(snr) %>%
          dplyr::summarize_(.dots = setNames(obj, nms)) %>%
          dplyr::ungroup()
      })

      # melt the data on which move and accumulate
      dfObs <- NULL

      for (mv in moves) {
        dfMv <- dfStats %>%
          dplyr::select(snr, starts_with(mv), N) %>%
          dplyr::rename_(.dots = setNames(names(.), gsub(concat(mv, "\\.", b, "\\."), "", names(.)))) %>%
          dplyr::mutate(move = mv)
        dfObs <- rbind(dfObs, dfMv)
      }

      dfObs$obs <- b
      dfCh <- rbind(dfCh, dfObs)
    }

    dfCh$exp <- channel
    dfCh
  }

  # collect the results over each expected match base
  dfErr = list()

  # Check if the input condition names match the number of csv files
  readcondition = report$condition.table$Condition
  if (length(readcondition) != length(errormodeList)) {
    stop("The number of errormode csv files does not match the number of conditions.")
  }

  for (i in 1:length(readcondition)) {
    dfErr[[i]] = NA
    for (b in bases) {
      dfErr[[i]] <- rbind(dfErr[[i]], massage(errormodeList[[i]], b))
      dfErr[[i]] = dfErr[[i]][c(2:nrow(dfErr[[i]])),]
    }
    # make sure things are factors and the appropriate levels
    levels(dfErr[[i]]$snr) <- head(breaks, length(levels(dfErr[[i]]$snr)))
    dfErr[[i]]$move <- as.factor(dfErr[[i]]$move)
    dfErr[[i]]$exp <- as.factor(dfErr[[i]]$exp)
    dfErr[[i]]$obs <- as.factor(dfErr[[i]]$obs)
    dfErr[[i]] <- as.data.frame(dfErr[[i]])
    row.names(dfErr[[i]]) <- seq.int(nrow(dfErr[[i]]))
    # Add Condition name from the input file names
    dfErr[[i]]$Condition = readcondition[i]
  }
  # write the data
  dfErr = do.call(rbind, dfErr)
  report$write.table("FishboneSnrBinnedSummary.csv",
                     dfErr,
                     id = "fishbone_snr_binned_summary",
                     title = "Fishbone Snr Binned Summary")

  ## WRITE THE PLOTS ##
  # filter by min number of samples in each SNR bin
  dfErr_ <- dfErr %>% filter(N >= minSample)
  loginfo("filtered %d of %d samples", nrow(dfErr) - nrow(dfErr_), nrow(dfErr))
  if (nrow(dfErr_) == 0) {
    stop("You have filtered out all of your data and need to increase the Min Sample!")
  }

  t <- function(mv) ifelse(mv == "Match", "Mismatch", as.character(mv))

  dfErr_ <- dfErr_ %>%
    dplyr::mutate(exp_ = concat("NextBase - ", exp), obs_ = concat(t(move), " - ", obs))
  dfErr_$exp_ <- as.factor(dfErr_$exp_)
  dfErr_$obs_ <- as.factor(dfErr_$obs_)

  limits <- function(df) {
    m <- floor(min(df$mu - df$ci) * 50) / 50
    M <- ceiling(max(df$mu + df$ci) * 50) / 50
    c(m, M)
  }

  labelFn <- function(x) { sprintf("%0.2f", x) }
  breaksFn <- function(x) { seq(x[[1]], x[[2]], by = 0.02)}

  dfIns <- dfErr_ %>% filter(move == "Insert")
  ylim <- limits(dfIns)
  breaks <- breaksFn(ylim)
  tp = ggplot(dfIns, aes(x = as.numeric(as.character(snr)), y = mu, group = Condition, colour = Condition)) +
    geom_point(aes(colour = Condition)) + geom_line(aes(colour = Condition)) + geom_errorbar(aes(ymin = mu - ci, ymax = mu + ci, colour = Condition), width = 0.1, position = pd) +
    xlim(0, 20) + scale_y_continuous(limits = ylim, breaks = breaks) +
    labs(x = "SNR by Outcome", y = "HMM Insertion Error") +
    facet_grid(exp_ ~ obs_) +
    plTheme + clScale + themeTilt
  report$ggsave(
    "fishboneplot_insertion.png",
    tp,
    id = "fishboneplot_insertion",
    title = "FishbonePlot - Insertion",
    caption = "FishbonePlot - Insertion"
  )

  dfMM <- dfErr_ %>% filter(move == "Match" & obs != exp)
  ylim <- limits(dfMM)
  breaks <- breaksFn(ylim)
  tp = ggplot(dfMM, aes(x = as.numeric(as.character(snr)), y = mu, group = Condition, colour = Condition)) +
    geom_point(aes(colour = Condition)) + geom_line(aes(colour = Condition)) + geom_errorbar(aes(ymin = mu - ci, ymax = mu + ci, colour = Condition), width = 0.1, position = pd) +
    xlim(0, 20) + scale_y_continuous(limits = ylim, breaks = breaks) +
    labs(x = "SNR by Outcome", y = "HMM Mismatch Error") +
    facet_grid(exp_ ~ obs_) +
    plTheme + clScale + themeTilt
  report$ggsave(
    "fishboneplot_mismatch.png",
    tp,
    id = "fishboneplot_mismatch",
    title = "FishbonePlot - MisMatch",
    caption = "FishbonePlot - MisMatch"
  )

  dfDel <- dfErr_ %>% filter(move == "Dark" | move == "Merge")
  ylim <- limits(dfDel)
  breaks <- breaksFn(ylim)
  tp = ggplot(dfDel, aes(x = as.numeric(as.character(snr)), y = mu, colour = Condition, group = Condition)) +
    geom_point(aes(colour = Condition)) + geom_line(aes(colour = Condition)) + geom_errorbar(aes(ymin = mu - ci, ymax = mu + ci, colour = Condition), width = 0.1, position = pd) +
    xlim(0, 20) + scale_y_continuous(limits = ylim, breaks = breaks) +
    labs(x = "SNR by Event", y = "HMM Deletion Error") +
    facet_grid(exp_ ~ move) +
    plTheme + clScale + themeTilt
  report$ggsave(
    "fishboneplot_deletion.png",
    tp,
    id = "fishboneplot_deletion",
    title = "FishbonePlot - Deletion",
    caption = "FishbonePlot - Deletion"
  )

  dfErr
}

makeReport <- function(report) {

  conditions = report$condition.table
  n = length(levels(conditions$Condition))
  clFillScale <<- getPBFillScale(n)
  clScale <<- getPBColorScale(n)

  # Commented out test data
  # condFile = "/pbi/dept/secondary/siv/smrtlink/smrtlink-internal/userdata/jobs-root/005/005578/tasks/pbcommandR.tasks.pbiplot_reseq_condition-0/resolved-tool-contract.json"
  # condjson = jsonlite::fromJSON(condFile)
  # input = condjson$resolved_tool_contract$input_files
  # decoded <- loadReseqConditionsFromPath(input)
  # conds = decoded@conditions
  # tmp = lapply(conds, function(z) data.frame(condition = z@condId,subreadset = z@subreadset, alignmentset = z@alignmentset, referenceset = z@referenceset))
  # testconditions = do.call(rbind, tmp)
  # testconditions = testconditions[c(1,3),]

  # Generate constant Arrow CSV files
  errormodeList = lapply(1:n, function(i) {
    loginfo(paste("Making constant Arrow CSV file for ", conditions$Condition[i], sep = ''))
    constantArrow(as.character(conditions$MappedSubreads[i]), as.character(conditions$Reference[i]), as.character(conditions$Condition[i]), report)
    # constantArrow(as.character(testconditions$alignmentset[i]), pbbamr::getReferencePath(as.character(testconditions$referenceset[i])), as.character(testconditions$condition[i]), report)
  })

  # Make Fishbone Plots
  makeFishbonePlots(errormodeList, report)

  # Save the report object for later debugging
  save(report, file = file.path(report$outputDir, "report.Rd"))

  # At the end of this function we need to call this last, it outputs the report
  report$write.report()
}

main <- function()
{
  report <- bh2Reporter(
    "condition-table.csv",
    "reports/ConstantArrowFishbonePlots/report.json",
    "Constant Arrow Fishbone Plots")
  makeReport(report)
  0
}

## Leave this as the last line in the file.
logging::basicConfig()
main()
